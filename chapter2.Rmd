[Exercise Set 2](./Exercise2.html).

### Data Wrangling

I created "data" folder, and added R script "create_learning2014.R" in the folder. 
The script reads the learning2014 data from http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt, and creates an analysis dataset which is saved in the data folder as learning2014.csv. 

### Analysis

First we download the learning2014 data. In the file header is given in the first row, and the data separator is ",".

```{r}
# read data
students2014 <- read.table("data/learning2014.csv", header = T, sep = ",")
# print dimesions of the table
dim(students2014)
# print first 6 rows
head(students2014)
```

Dataset has 7 variables and 166 properties (rows). The variables are:

```{r, echo=FALSE}
colnames(students2014)
```

Gender is `M` (Male), or `F` (Female), and age is given in years. Attitude describes the students total attitude toward statistics. Points are students' exam results. `deep`, `surf`, and `stra` are averages taken over the students answers to questions on deep, surface and strategic learning.

------------------------------------------------------------------------

Next, we can plot graphical overview of the data using `GGally` and `ggplot2` libraries. The variables are plotted separated in terms of gender.

```{r}
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)

# create a more advanced plot matrix with ggpairs()
p <- ggpairs(students2014[-1], mapping = aes(col = students2014$gender, alpha = 0.3), 
             lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
```

The plots in the diagonal show the density distributions of each value (red for female, blue for male). Scatter plots below the diagonal show how each pair of variables are related. 

Above the diagonal are shown total and gender specific correlations between each pair of variables. The number of stars '*' next to each value tells how statistically significant the correlations are.
The largest total correlation is between `attitude` and `points`. Interestingly, for male there is a very negative correlation between `deep` and `surf`, while for female these two are completely uncorrelated.

------------------------------------------------------------------------

We see that `attitude`, `stra`, and `surf` have the largest correlation with `points`. Thus, we choose them as explanatory variables and fit them to a regression model with exam points as the target variable.

```{r}
# create a regression model with multiple explanatory variables
m <- lm(points ~ attitude + stra + surf, data = students2014)

# print out a summary of the model
summary(m)
```
Looking at the summary, `surf` is does not have a statistically significant relationship (i.e. p-value is too large) with the target variable.
We can remove it and refit the model.

```{r}
m2 <- lm(points ~ attitude + stra, data = students2014)
summary(m2)
```

P-values for the remaining variables are now smaller, and the model's explanatory power is better.
There is strong positive correlation between `attitude` and the target variable, but `stra` (strategic learning) has also a small effect.

Multiple R-squared tells how well the variables explain the variation of the target. In this case only 20.48 % of variation of exam points is explained by `attitude` and `stra`. The explanatory power of the model is quite low.

------------------------------------------------------------------------

We can use the model produce the following diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage. 

```{r}
# Plot residuals vs fitted values
plot(m2, which = c(1))
```

The model assumes a linear regression. If the "Residuals vs Fitted" plot were curved, we might need to include a quadratic term in the model. Based on the plot, a linear model is sufficient. Points 56, 35, and 145 are the biggest outliers.

```{r}
# Plot Q-Q residuals
plot(m2, which = c(2))
```

"Q-Q Residuals" follows fairly closely to the straight line apart from a few outliers. This shows that the prediction errors are close to a normal distribution.

```{r}
# Plot residuals vs Leverage
plot(m2, which = c(5))
```

Leverage is a measure of how much influence each point has on the model prediction. In "Residuals vs Leverage", we see that all leverage values are small, but some points have a proportionally larger effect.


