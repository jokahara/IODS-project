[Exercise Set 4](exercises/Exercise4.html).

### Analysis

First we load the Boston data from the MASS package.

```{r, echo=FALSE}
library(MASS)
library(ggplot2)
data("Boston")
set.seed(10) # make sure results are always same
```

Boston dataset consists of information collected by the U.S Census Service concerning housing in the Boston suburbs, such as per capita crime rate (`crim`). The data frame has 506 rows and 14 columns. 

```{r}
# print variable names and dimensions of the data
str(Boston)
```

------------------------------------------------------------------------

We can visualize the relationships between variables by calculating and plotting a correlation matrix:

```{r}

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 
round(cor_matrix, digits = 2)

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle")

```

For example, crime correlates the most with accessibility to radial highways (`rad`) and property-tax rate (`tax`).

Summarising the data variables:

```{r}
summary(Boston)
```

For each variable `summary` prints out the mean, median, minimum and maximum values, and the 1st and 3rd quartiles.

------------------------------------------------------------------------

Next we want to standardize the dataset so that them mean is 0 and standard deviation is 1 for each variable.
```{r}
# center and standardize variables
boston_scaled <- as.data.frame(scale(Boston))
# make crime rate variable numeric
boston_scaled$crim <- as.numeric(boston_scaled$crim)
summary(boston_scaled)
```

We can create a categorical variable of the crime rate by dividing the scaled crime rate between the four quantiles.

```{r}
# divide data to bins
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# print table of the new variable
table(crime)

# drop the old crime rate variable from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Finally, we divide the dataset to train and test sets with 80% of the data in the train set:
```{r}
# choose randomly 80% of the rows
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)

# create train and test set
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

```

------------------------------------------------------------------------

### Linear Discriminant Analysis

We use linear discriminant analysis on the train set with crime rate as the target variable and all the other variables as predictor variables. 

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

------------------------------------------------------------------------

We can test the accuracy of the LDA model with `predict` and cross tabulate the results.

```{r}
# separate the correct classes from test data
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)

```

66 / 102 = 64.7% of classes were correctly predicted. High and low crime rate predictions are mostly correct, while the middle crime rate prediction accuracy is quite low.

------------------------------------------------------------------------

### K-Means Clustering

```{r, echo=FALSE}
# reload Boston data
data("Boston")
set.seed(1)
# center and standardize variables
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled$crim <- as.numeric(boston_scaled$crim)
```

Similarity between measurements can be measured by calculating the euclidian distances between observation:
```{r}
# euclidean distance matrix
dist_eu <- dist(Boston)
summary(dist_eu)
```

K-means clustering divides the dataset into k clusters based on their distances.

```{r}
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

Optimal number of clusters is found when the total of within cluster sum of squares (WCSS) is drops radically.
In this case the optimal k is 2. Plotting the cluster in part of the dataset, separated by colors:

```{r}
# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot part of the dataset with clusters
pairs(Boston[c("rm", "age", "dis", "crim")], col = km$cluster)

```

------------------------------------------------------------------------


### **Bonus** 

K-means clustering with `k=3`:

```{r}
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled$crim <- as.numeric(boston_scaled$crim)

km <- kmeans(boston_scaled, centers = 3)
pairs(boston_scaled[c("rm", "age", "dis", "crim")], col = km$cluster)
```

Perform LDA using the clusters as target classes:

```{r}
train$crime <-as.numeric(train$crime)
km <- kmeans(train, centers = 3)
# linear discriminant analysis
lda.fit <- lda(km$cluster ~ ., data = train)
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crim)

# plot the lda results
plot(lda.fit, dimen = 2, col = km$cluster, pch = km$cluster)
lda.arrows(lda.fit, myscale = 1)

```

The most influential linear separators for the clusters are `tax` and `rad`.

------------------------------------------------------------------------

### Data Wrangling (for next week)

Added `create_human.R` to `data` folder, which downloads “Human development” and “Gender inequality” datasets, joins them and saves the data to `human.csv`.


------------------------------------------------------------------------









